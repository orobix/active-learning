{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _MNIST_ Active Learning example\n",
    "\n",
    "This notebook contains a simple example of how to implement the Active Learning framework using modAL and the query strategies of this repository. For this we are going to use a subset of [_MNIST_](https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits) dataset, loaded from _sklearn_.\n",
    "First we import the query strategies that we are going to compare and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from modAL.disagreement import consensus_entropy_sampling, max_disagreement_sampling, vote_entropy_sampling\n",
    "from modAL.uncertainty import entropy_sampling, margin_sampling, uncertainty_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from activelearning.AL_cycle import plot_results, strategy_comparison\n",
    "from activelearning.queries.representative.coreset_query import query_coreset\n",
    "from activelearning.queries.representative.probcover_query import query_probcover\n",
    "from activelearning.queries.representative.random_query import query_random\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to tensors of acceptable sizes for ResNet and normalize\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# import data\n",
    "train_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_full = datasets.MNIST(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "# we are going to use a subset of 1500 images for this example\n",
    "y_train = train_full.targets[:1000,]\n",
    "y_test = test_full.targets[:500,]\n",
    "\n",
    "train = Subset(train_full, indices=range(1000))\n",
    "test = Subset(test_full, indices=range(500))\n",
    "\n",
    "# to use batches instead of one at a time\n",
    "loaded_train = DataLoader(train, batch_size=64, shuffle=False)\n",
    "loaded_test = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MNIST_ consists of images of handwritten digits from 0 to 9. When working with image data, we need to first extract features using for example a pretrained model. In this case, we will use [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ResNet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# remove classification layer\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "# set to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.to(torch.device(device))\n",
    "\n",
    "train_features_list = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in loaded_train:\n",
    "        features_by_resnet = resnet(images.to(device))\n",
    "        train_features_list.append(features_by_resnet)\n",
    "\n",
    "test_features_list = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in loaded_test:\n",
    "        features_by_resnet = resnet(images.to(device))\n",
    "        test_features_list.append(features_by_resnet)\n",
    "\n",
    "\n",
    "train_features = torch.cat(train_features_list, dim=0).squeeze()\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "\n",
    "test_features = torch.cat(test_features_list, dim=0).squeeze()\n",
    "test_features = test_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of a randomforest classifier on the complete training set. This will serve as reference metric for the active learning query strategies, as we want to reach the same accuracy but with less labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_mod = RandomForestClassifier()\n",
    "RF_mod.fit(train_features, y_train)\n",
    "\n",
    "goal_acc = RF_mod.score(test_features, y_test)\n",
    "print(goal_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how different query strategies perform, we can use the *strategy_comparison* function and pass the strategies to be used. We can also pass more than one number of instances, to check whether a different batch size influences performance. *plot_results* can be used to immediatly plot the output from *strategy_comparison*, or a custom graph can be created from the scores data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [16, 32]\n",
    "\n",
    "scores = strategy_comparison(\n",
    "    X_train=None,\n",
    "    y_train=None,\n",
    "    X_pool=train_features,\n",
    "    y_pool=y_train,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategies=[\n",
    "        uncertainty_sampling,\n",
    "        margin_sampling,\n",
    "        entropy_sampling,\n",
    "        query_random,\n",
    "        query_coreset,\n",
    "        query_probcover,\n",
    "    ],\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a random starting set for the initial training of the model, instead of immediatly starting with the query strategies, we can simply pass the relative argument to the *strategy_comparison* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, train_pool, y_start, y_pool = train_test_split(train_features, y_train, test_size=0.75)\n",
    "\n",
    "n_instances = [16, 32]\n",
    "\n",
    "scores_v2 = strategy_comparison(\n",
    "    X_train=train_start,\n",
    "    y_train=y_start,\n",
    "    X_pool=train_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategies=[\n",
    "        uncertainty_sampling,\n",
    "        margin_sampling,\n",
    "        entropy_sampling,\n",
    "        query_random,\n",
    "        query_coreset,\n",
    "        query_probcover,\n",
    "    ],\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores_v2,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a committee of models for the selection of query instances. To do so, we can use the *committee_classifiers* to specify which models we want to form a committee. Query strategies in this case should be appropriate to the committee framework. In this example we use a committee of two simple neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_qbc = strategy_comparison(\n",
    "    X_train=train_start,\n",
    "    y_train=y_start,\n",
    "    X_pool=train_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategies=[vote_entropy_sampling, consensus_entropy_sampling, max_disagreement_sampling, query_random],\n",
    "    committee_classifiers=[\"nnet\", \"nnet\"],\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores_qbc,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a different classifier instead of a random forest, we can simply change the *classifier* argument of *strategy_comparison*. For example, we can use a simple neural network classifier instead. If the default options aren't sophisticated enough, a custom model can be passed as input, as long as it comes from *sklearn* or one of its wrappers. For neural networks, you can use *skorch* and refer to the modAL documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [16, 32]\n",
    "\n",
    "scores_nnet = strategy_comparison(\n",
    "    X_train=train_start,\n",
    "    y_train=y_start,\n",
    "    X_pool=train_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"nnet\",\n",
    "    query_strategies=[\n",
    "        uncertainty_sampling,\n",
    "        margin_sampling,\n",
    "        entropy_sampling,\n",
    "        query_random,\n",
    "        query_coreset,\n",
    "        query_probcover,\n",
    "    ],\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores_nnet,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
