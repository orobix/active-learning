{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream based scenario - _MNIST_ example\n",
    "\n",
    "This notebook contains an example on how to implement the stream based scenario of Active Learning. For this we are going to use a subset of [_MNIST_](https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits) dataset, loaded from _sklearn_.\n",
    "Stream based AL can work in two settings:\n",
    "- in batch setting, we store the points as they arrive from a stream until a batch is complete, and then we treat this batch as the pool of the pool based scenario, by applying the standard query strategies.\n",
    "- in stream setting, we evaluate one point at a time and use tailored strategies to decide whether to annotate this point or discard it.\n",
    "Both settings can be implemented with our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from activelearning.AL_cycle import plot_results, strategy_comparison\n",
    "from activelearning.queries.informative.margin_query_stream import stream_query_margin\n",
    "from activelearning.queries.representative.coreset_query import query_coreset\n",
    "from activelearning.queries.representative.diversity_query_stream import stream_query_diversity\n",
    "from activelearning.queries.representative.random_query import query_random\n",
    "from activelearning.queries.representative.random_query_stream import stream_query_random\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to tensors of acceptable sizes for ResNet and normalize\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# import data\n",
    "train_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=preprocess)\n",
    "test_full = datasets.MNIST(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "\n",
    "# we are going to use a subset of 1500 images for this example\n",
    "y_train = train_full.targets[:1000,]\n",
    "y_test = test_full.targets[:500,]\n",
    "\n",
    "train = Subset(train_full, indices=range(1000))\n",
    "test = Subset(test_full, indices=range(500))\n",
    "\n",
    "# to use batches instead of one at a time\n",
    "loaded_train = DataLoader(train, batch_size=64, shuffle=False)\n",
    "loaded_test = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MNIST_ consists of images of handwritten digits from 0 to 9. When working with image data, we need to first extract features using for example a pretrained model. In this case, we will use [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ResNet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# remove classification layer\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "# set to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.to(torch.device(device))\n",
    "\n",
    "train_features_list = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in loaded_train:\n",
    "        features_by_resnet = resnet(images.to(device))\n",
    "        train_features_list.append(features_by_resnet)\n",
    "\n",
    "test_features_list = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in loaded_test:\n",
    "        features_by_resnet = resnet(images.to(device))\n",
    "        test_features_list.append(features_by_resnet)\n",
    "\n",
    "\n",
    "train_features = torch.cat(train_features_list, dim=0).squeeze()\n",
    "\n",
    "train_features = train_features.cpu().numpy()\n",
    "\n",
    "test_features = torch.cat(test_features_list, dim=0).squeeze()\n",
    "test_features = test_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating performance in the stream based scenario is trickier than in pool based scenario, as the random order in which points are sampled (and batches are formed) will heavily influence the result. We can still use the complete training set accuracy as reference metric, but keeping in mind that some important points might be discarded (for example if a batch is formed of mostly very relevant points) and therefore the goal accuracy might never be reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_mod = RandomForestClassifier()\n",
    "RF_mod.fit(train_features, y_train)\n",
    "\n",
    "goal_acc = RF_mod.score(test_features, y_test)\n",
    "print(goal_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the query strategy in the batch setting, we simply need to specify the *batch_size* parameter in the *strategy_comparison* function. In this case, the choice of *n_instances* is very important, as it represents how much of the batch will be kept and how much will be discarded. Choosing to keep a high percentage of instances will result in an accuracy level closer to the goal, but the potential saving in labeling costs is reduced, while choosing a low percentage will guarantee less labeling, but it might fail to reach the same levels of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [16, 32]\n",
    "\n",
    "scores_batch = strategy_comparison(\n",
    "    X_train=None,\n",
    "    y_train=None,\n",
    "    X_pool=train_features,\n",
    "    y_pool=y_train,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategies=[uncertainty_sampling, query_coreset, query_random],\n",
    "    batch_size=64,\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores_batch,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the stream setting instead, we simply need to specify *n_instances* equal to 1. Given the random element of the stream, it might be a good idea to replicate the experiment more than once, to get a sense of the variability. We usually want to keep all the first *n* points coming from the stream, to create a solid base of labeled points before employing the query strategies; this can be done either by provinding a starting set (*X_train*, *y_train* parameters) or by specifying the *start_len* parameter. The choice of *quantile* indicates how high the threshold for the decision of accepting or discarding a point will be, according to the query strategy. More detail on the theshold can be found in each query's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [1, 1]\n",
    "\n",
    "scores_stream = strategy_comparison(\n",
    "    X_train=None,\n",
    "    y_train=None,\n",
    "    X_pool=train_features,\n",
    "    y_pool=y_train,\n",
    "    X_test=test_features,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    start_len=100,\n",
    "    quantile=0.5,\n",
    "    query_strategies=[stream_query_margin, stream_query_diversity, stream_query_random],\n",
    "    n_instances=n_instances,\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores_stream,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=train_features.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
