{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Iris_ Active Learning example\n",
    "\n",
    "This notebook contains a simple example of how to implement the Active Learning framework using modAL and the query strategies of this repository. For this we are going to use the [_Iris_](https://archive.ics.uci.edu/dataset/53/iris) dataset, loaded from _sklearn_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from activelearning.AL_cycle import cycle_AL, plot_results, strategy_comparison\n",
    "from activelearning.queries.representative.coreset_query import query_coreset\n",
    "from activelearning.queries.representative.density_query import query_density\n",
    "from activelearning.queries.representative.kmeans_query import query_kmeans_foreach\n",
    "from activelearning.queries.representative.random_query import query_random\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris[\"data\"]  # attributes\n",
    "y = iris[\"target\"]  # labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "X_pool, y_pool = X_train, y_train\n",
    "# use the next rows to split the training set in initial set and pool\n",
    "# X_start, X_pool, y_start, y_pool = train_test_split(X_train, y_train, test_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data in a training set and a test set, we evaluate the performance of a randomforest classifier on the complete training set. This will serve as reference metric for the active learning query strategies, as we want to reach the same accuracy but with less labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_mod = RandomForestClassifier()\n",
    "RF_mod.fit(X_train, y_train)\n",
    "\n",
    "goal_acc = RF_mod.score(X_test, y_test)\n",
    "print(goal_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement the Active Learning cycle for pool based sampling using the *cycle_AL* function. We can specify any of the query strategies for pool based sampling from modAL or from this repository in the learner instantiation, as well as other paramaters like the number of data points to query at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, instances = cycle_AL(\n",
    "    X_train=None,\n",
    "    y_train=None,\n",
    "    X_pool=X_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategy=query_random,\n",
    "    goal_acc=goal_acc,\n",
    ")\n",
    "# accuracies contains the accuracy score at each iteration\n",
    "# instances contains the count of labeled instances at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize result\n",
    "\n",
    "plt.plot(instances, accuracies)\n",
    "plt.title(\"Accuracy over instances\")\n",
    "plt.xlabel(\"Instances\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.axhline(y=goal_acc, color=\"y\", linestyle=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how different query strategies perform, we can use the *strategy_comparison* function and pass the strategies to be used. We can also pass more than one number of instances, to check whether a different batch size influences performance. *plot_results* can be used to immediatly plot the output from *strategy_comparison*, or a custom graph can be created from the scores data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [3]  # put [3, 9, 18] to confront different batch sizes\n",
    "\n",
    "scores = strategy_comparison(\n",
    "    X_train=None,\n",
    "    y_train=None,\n",
    "    X_pool=X_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    classifier=\"randomforest\",\n",
    "    query_strategies=[query_kmeans_foreach, query_density, query_coreset, query_random],\n",
    "    n_instances=n_instances,\n",
    "    K=3,  # number of clusters for k-means query\n",
    "    metric=\"euclidean\",  # metric for density query\n",
    "    goal_acc=goal_acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=X_pool.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(7, 4),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
