{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian MC dropout query strategies\n",
    "\n",
    "Bayesian query strategies use Monte Carlo (MC) dropout to approximate uncertainty in deep learning models. This works by computing multiple forward passes through a neural network with the dropout layers activated. For this example we are going to use a subset of [_MNIST_](https://archive.ics.uci.edu/dataset/683/mnist+database+of+handwritten+digits) dataset, loaded from _sklearn_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from activelearning.AL_cycle import plot_results, strategy_comparison\n",
    "from activelearning.queries.bayesian.mc_bald import mc_bald\n",
    "from activelearning.queries.bayesian.mc_max_entropy import mc_max_entropy\n",
    "from activelearning.queries.bayesian.mc_max_meanstd import mc_max_meanstd\n",
    "from activelearning.queries.bayesian.mc_max_varratios import mc_max_varratios\n",
    "from activelearning.queries.representative.random_query import query_random\n",
    "from activelearning.utils.skorch_nnet import reshapedVGG\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are going to run the images though a [VGG16](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html) neural network, as this architecture includes dropout layers. It already includes a feature extraction part, so we can pass images directly to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(\"./data\", download=True, transform=preprocess)\n",
    "dataloader = DataLoader(mnist_data, shuffle=True, batch_size=1500)\n",
    "X, y = next(iter(dataloader))\n",
    "\n",
    "# read training data (subset of 1500 images from mnist)\n",
    "X_train, X_test, y_train, y_test = X[:1000], X[1000:1500], y[:1000], y[1000:1500]\n",
    "X_train = X_train.reshape(1000, -1)\n",
    "X_test = X_test.reshape(500, -1)\n",
    "\n",
    "# assemble initial data\n",
    "n_initial = 100\n",
    "initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "\n",
    "\n",
    "# generate the pool\n",
    "# remove the initial data from the training dataset\n",
    "X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "y_pool = np.delete(y_train, initial_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of the VGG classifier that we are going to use on the complete training set. This will serve as reference metric for the active learning query strategies, as we want to reach the same accuracy but with less labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetClassifier(\n",
    "    reshapedVGG(num_classes=10),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    max_epochs=15,\n",
    "    device=device,\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "goal_acc = classifier.score(X_test, y_test)\n",
    "print(goal_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how different query strategies perform, we can use the *strategy_comparison* function and pass the strategies to be used. We can also pass more than one number of instances, to check whether a different batch size influences performance. *plot_results* can be used to immediatly plot the output from *strategy_comparison*, or a custom graph can be created from the scores data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = [32]\n",
    "\n",
    "scores = strategy_comparison(\n",
    "    X_train=X_initial,\n",
    "    y_train=y_initial,\n",
    "    X_pool=X_pool,\n",
    "    y_pool=y_pool,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    classifier=\"nnet_bo\",\n",
    "    query_strategies=[mc_bald, mc_max_entropy, mc_max_varratios, query_random, mc_max_meanstd],\n",
    "    n_instances=[32],\n",
    "    goal_acc=goal_acc,\n",
    "    max_epochs=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    scores,  # output data frame from strategy_comparison\n",
    "    n_instances=n_instances,\n",
    "    tot_samples=X_train.shape[0],  # size of the original training set, for scale\n",
    "    goal_acc=goal_acc,\n",
    "    figsize=(7, 4),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
